{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from functools import reduce\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(\"1.csv\")\n",
    "b = pd.read_csv(\"2.csv\")\n",
    "c = pd.read_csv(\"3.csv\")\n",
    "# drop redundant index column\n",
    "a.drop(a.columns[[0]], axis=1, inplace=True)\n",
    "b.drop(b.columns[[0]], axis=1, inplace=True)\n",
    "c.drop(c.columns[[0]], axis=1, inplace=True)\n",
    "# print(a.head())\n",
    "# print(b.head())\n",
    "# print(c.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to find duplicate entires (ngos that were listed on multiple websites)\n",
    "# we are going to use the URL\n",
    "\n",
    "def getNetLoc(row):\n",
    "    website = row['website']\n",
    "    if isinstance(website, str):\n",
    "        website = urlparse(website)\n",
    "        return website[1]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['netloc'] = a.apply(getNetLoc, axis=1)\n",
    "b['netloc'] = b.apply(getNetLoc, axis=1)\n",
    "c['netloc'] = c.apply(getNetLoc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(a.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Were gonna merge on netloc so if the row doesnt have one remove it to be added later\n",
    "a2 = a[a['netloc'] != \"\"]\n",
    "b2 = b[b['netloc'] != \"\"]\n",
    "c2 = c[c['netloc'] != \"\"]\n",
    "a3 = a[a['netloc'] == \"\"]\n",
    "b3 = b[b['netloc'] == \"\"]\n",
    "c3 = c[c['netloc'] == \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [a2, b2, c2]\n",
    "# The line below should work, but some of the websites that were sourced from had duplicate entries of their own\n",
    "# causing the validate line to fail. guess it wont be so horrible if my code results in some duplicates ¯\\_(ツ)_/¯\n",
    "# merged = reduce(lambda left,right: pd.merge(left, right, on='netloc', how='outer', validate='many_to_one'), dfs)\n",
    "merged = reduce(lambda left, right: pd.merge(left, right, on='netloc', how='outer'), dfs)\n",
    "# print(merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are merging in the ngos that didnt have a website, just hoping they arent duplicates\n",
    "dfs = [merged, a3, b3, c3]\n",
    "\n",
    "final = reduce(lambda left, right: pd.merge(left, right, how='outer'), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output\n",
    "final.to_csv(\"merged.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
