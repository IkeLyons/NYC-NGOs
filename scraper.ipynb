{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs4 for scraping, requests for requesting, pandas for data-ing\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First website to be scraped\n",
    "URL1 = 'https://www.nonprofitnewyork.org/join/directory/'\n",
    "page = requests.get(URL1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape relevant HTML\n",
    "soup1 = BeautifulSoup(page.content, 'html.parser')\n",
    "all_non_profits = soup1.select('.tab-pane > ul > li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process scraped HTML and extract website + non profite name\n",
    "nps = []\n",
    "\n",
    "for np in all_non_profits:\n",
    "    a = np.find('a')\n",
    "    if a:\n",
    "        nps.append({'website': a['href'], 'name': a.contents[0]})\n",
    "    else:\n",
    "        # text of the non linked nps is the second child of the li tag\n",
    "        nps.append({'name': np.contents[1]})\n",
    "\n",
    "# Save the data\n",
    "df1 = pd.DataFrame(nps)\n",
    "# print(df1.head())\n",
    "df1.to_csv('1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onto the next site, should be a similar process to above\n",
    "URL2 = 'https://queensbp.org/nonprofits/'\n",
    "page = requests.get(URL2)\n",
    "soup2 = BeautifulSoup(page.content, 'html.parser')\n",
    "all_non_profits2 = soup2.select('#tablepress-1 > tbody > tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nps2 = []\n",
    "\n",
    "for np in all_non_profits2:\n",
    "    name, websiteCol = np.find_all('td')\n",
    "    website = websiteCol.find('a')\n",
    "    row = {'name': name.contents[0]}\n",
    "    if website:\n",
    "        row['website'] = website.contents[0]\n",
    "    nps2.append(row)\n",
    "\n",
    "df2 = pd.DataFrame(nps2)\n",
    "df2.to_csv('2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Site - note the large 100000 argument in the url to get all listings on one page\n",
    "URL3 = 'https://www.nycservice.org/organizations/index.php?sort_mode=title_asc&max_records=100000'\n",
    "page = requests.get(URL3)\n",
    "soup3 = BeautifulSoup(page.content, 'html.parser')\n",
    "# The indexing at the end cuts off the first entry as it is the table header, not an NGO\n",
    "all_non_profits3 = soup3.select('.data > tr')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "nps3 = []\n",
    "\n",
    "for np in all_non_profits3:\n",
    "    nameCol, descriptionCol = np.find_all('td')\n",
    "    nameA = nameCol.find('a')\n",
    "    fullInfoPage = 'https://www.nycservice.org' + nameA['href']\n",
    "    row = {'name': nameA.contents[0], 'description': descriptionCol.contents[0]}\n",
    "    \n",
    "    # Each ngo has its own page on the website with more useful info to scrape\n",
    "    companyPage = requests.get(fullInfoPage)\n",
    "    tempSoup = BeautifulSoup(companyPage.content, 'html.parser')\n",
    "    \n",
    "    website = tempSoup.select('.orginfo > a')\n",
    "    if(website):\n",
    "        row['website'] = website[0]['href']\n",
    "   \n",
    "    address = tempSoup.select('.address > a')\n",
    "    if address: \n",
    "        addressText = []\n",
    "        for line in address[0].contents:\n",
    "            if str(line) != '<br/>':\n",
    "                addressText.append(line)\n",
    "        row['address'] = ' '.join(addressText)\n",
    "    # print(row)\n",
    "    nps3.append(row)\n",
    "\n",
    "df3 = pd.DataFrame(nps3)\n",
    "df3.to_csv('3.csv')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20bf69066c0dd38d51965b69d5e1b6e387082e3198ba56e97997ac55f4e50ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
